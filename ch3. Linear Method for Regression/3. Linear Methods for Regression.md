# 3. Linear Method for Regression

## caution

except for 3.7. ~ the end

## 3.1. Introduction

Linear Method는 과거부터 현재까지 널리 쓰이는 모델이다. 그리고, input이 output에 어떤 영향을 미치는지 설명하는 것이 쉬우며, training case가 적거나 signal-to-noise 비율이 낮거나 sparse data인 경우에는 non-linear 모델보다 성능이 좋다.
마지막으로, input 값들의 단위를 변환시키는데 사용되며 이를 'basis-function method'라고 부른다.

## 3.2. Linear Regression Models and Least Squares

<div ailgn="center">
<img src="imgs/eq_3_1.jpg" />
</div>

선형모델은 p개의 feature(![formular](https://render.githubusercontent.com/render/math?math=X^T=(X_1,X_2,...,X_p)))를 가지는 위의 수식으로 표현할 수 있다.

Xj로 위치할 수 있는 Data source

* quantitative inputs
* transformations of quantitiative inputs, such as log, squared-root or square
* basis expansions, such as ![formular](https://render.githubusercontent.com/render/math?math=X_2=X_1^2,X_3=X_1^3), leading to a polynomial representation
* numeric or "dummy" coding of the levels of quantitative inputs
* interactions between variables

Trainin data는 (x1,y1)의 쌍으로 ... (Xn,yn)과 같이 N개의 datapoint로 구성되어 있으며, 하나의 datapoint i에 대해서는 (xi1,xi2,...,xip)와 같이 p개의 column 값으로 구성된다. 그리고 estimation을 통해 추측해야 하는 beta의 값 또한 beta = (b0,b1,...bp)의 와 같이 구성된다.

beta를 estimation 하기 위해 가장 유명한 접근 방법은 RSS 방식이며, 이는 아래와 같다.

<div ailgn="center">
<img src="imgs/eq_3_2.jpg" />
</div>

training observation (xi,yi)는 모집단으로부터 random(임의로)하게 추출되는 것이 바람직한 방법이다. **그러나 xi가 임의로 추출되지 않더라도 yi가 주어진 xi에 대해 conditionally independent하다면 여전히 RSS 방법은 유효하다.**

====================================

### conditional independent

<a href="http://norman3.github.io/prml/docs/chapter08/2.html">http://norman3.github.io/prml/docs/chapter08/2.html</a>

세 개의 변수 a, b, c가 있다.
* 이 때 b와 c가 주어졌을 때 a에 대한 조건부확률을 계산해보자.
* a가 b에 독립적이라면 다음과 같은 식이 성립한다.
    
    <img src="https://latex.codecogs.com/svg.latex?p(a|b,c)=p(a|c) \qquad{(8.20)}" title="p(a|b,c)=p(a|c) \qquad{(8.20)}" />

    위의 경우를 c가 주어졌을 때 a는 b에 대해 조건부 독립이라고 표현한다.

* 이런 상황에서 c가 주어진 상태의 a와 b의 결합 분포는 다음과 같다.

    <img src="https://latex.codecogs.com/svg.latex?p(a,b|c) = p(a|b,c)p(b|c)=p(a|c)p(b|c) \qquad{(8.21)}" title="p(a,b|c) = p(a|b,c)p(b|c)=p(a|c)p(b|c) \qquad{(8.21)}" />

    이를 조건부 독립이라고 하고, 다음과 같이 표기한다.

    <img src="https://latex.codecogs.com/svg.latex?a\perp\!\!\!\!\!\perp b\;|\;c \qquad{(8.22)}" title="a\perp\!\!\!\!\!\perp b\;|\;c \qquad{(8.22)}" />

확률 변수의 조건부 독립여부는 중요한 속성이다.
* 사용되는 모델을 단순화시킬 수 있고,
* 학습에 필요한 연상량을 낮출 수 있다.

====================================

eq 3.2.는 linear model 3.1.에서 아무런 가정이 없는 수식이지만 Least square 방식으로 어느 정도는 fitting 가능하다.

X를 N X (p+1) matrix로, y를 N vector로 구성된 훈련셋의 output으로 따져볼 때, 어떻게 3.2. 식을 최소화할 수 있을까?

<div align="center">
<img src="imgs/eq_3_3.jpg" />
<img src="imgs/eq_3_4.jpg" />
</div>

3.3. 식을 beta에 대한 2차원식으로 나타내면, RSS를 최소로하는 beta를 계산하기 위해 beta에 대한 1차 및 2차 편미분을 식으로 나타내면 3.4.와 같다.

**X를 full column rank로 가정**하면 ![formular](https://render.githubusercontent.com/render/math?math=X^TX)는 양의 값을 가지고 1차 미분에 대해 0으로 하는 식은 아래와 같다. 

<div align="center">
<img src="imgs/eq_3_5.jpg" />
<img src="imgs/eq_3_6.jpg" />
</div>

그리고 이를 unique하게 하는 beta의 값은 위와 같다.

<div align="center">
<img src="imgs/eq_3_7.jpg" />
</div>

input vector x0에 대한 예측값은 ![formular](https://render.githubusercontent.com/render/math?math=\hat{f}(x_0)=(1:x_0)^T\hat{\beta})로 나타낼 수 있으며 training input으로 feat된 값은 위의 수식과 같다.

![formular](https://render.githubusercontent.com/render/math?math=H=X(T^TX)^{-1}X^Ty)는 y에 hat을 붙이기 위한 Matrix로 "hat" matrix라고도 불린다.

<div align="center">
<img src="imgs/fig_3_2.jpg" />
</div>

위의 그림은 least square 추정을 기하학적으로 나타내었을 때의 그림이며, x0는 1로 가정한 상태에서 x1과 x2 그리고 y에 대한 3차원 공간으로 투영했다. x1과 x2의 부분공간에 y의 직선이 추정치 y hat과 직교하도록 RSS를 최소화하는 beta를 찾아야 한다.

====================================

### rank and dimension in Linear algebra

<a href="https://losskatsu.github.io/linear-algebra/rank-dim/#">https://losskatsu.github.io/linear-algebra/rank-dim/#</a>

X의 컬럼들이 선형 독립이 아닌 경우(bot of full rank)가 발생할 수 있다(i.e. x2 = 3x1). 이 경우에는 ![formular](https://render.githubusercontent.com/render/math?math=X^TX)는 singular(역행렬을 가지지 않음)이며, least square의 beta hat은 unique한 값을 가질 수 없게 된다. 그러나 y hat은 여전히 X column들의 부분공간에 투영된다; X의 컬럼 벡터들의 관점에서 투영되는 것을 다른 방법으로 설명할 수 있다.

====================================

이러한 non-full rank case(컬럼간에 독립이 아닌 경우)는 qualitative input이 중복적으로 반영될 때 나타나며, 이를 위한 일반적인 방법은 반영을 새로하거나 X의 중복 컬럼을 제거하는 것이다. 

그리고 Rank Deficiency(training sample의 수보다 컬럼의 수가 더 많은 경우)의 경우에는 feature를 filtering이나 정규화를 반영한 fitting을 통해 해결한다.

**벡터 X의 컬럼이 상호 독립적이라는 가정을 가지고, beta hat을 추정하는데 yi간에 상관관계가 없고 ![formular](https://render.githubusercontent.com/render/math?math=\sigma^2)의 분산을 가지고 xi가 고정(non random)되어 있다고 가정하자.** Least sqaure 파라미터 추정을 위한 분산-공분산 matrix는 3.6과 아래의 3.8 수식을 통해 도출된다.

<div align="center">
<img src="imgs/eq_3_8.jpg" />
</div>

====================================

### 분산-공분산행렬 [variance-covariance matrix]

<a href="https://adioshun.gitbooks.io/statics-with-r/content/variance-covariance-matrix.html">https://adioshun.gitbooks.io/statics-with-r/content/variance-covariance-matrix.html</a>

k feature에 대한 정방행렬(k X k 행렬)을 각 변수의 분산과 공분산으로 채운 것.
* 대각행렬: 각 변수의 분산
* 이외의 값: 변수 사이의 공분산

====================================

3.6.과 3.8.을 가지고 yi에 대한 분산은 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_8_1.jpg" />
</div>

분모로 N 대신에 N-p-1를 하는 이유는 yi의 분산 추정량 sigma-hat-sqaure를 sigma-square의 불편추정량으로 만들기 위함이다(![formular](https://render.githubusercontent.com/render/math?math=\sigma^2:E(\hat{\sigma}^2)=\sigma^2)). 

====================================

### 불편추정량

<a href="http://www.aistudy.co.kr/math/estimate_lee.htm">http://www.aistudy.co.kr/math/estimate_lee.htm</a>

불편추정량은 편의가 없는 추정량이며, 이는 추정량이 모수에 대하여 큰 차이를 보이지 않고 어느 한쪽으로 치우침이 없다는 의미. 그러므로 불편추정량은 모수에 대하여 올바른 추정량이라는 의미이다.

i.e. 표본평균 및 표본비율에 대해서는 표본에 대한 기대값이 모수의 추정량과 같으므로 불편추정량이다.

====================================

표본분산은 편차의 제곱합을 N-1로 나누는데, 이는 표본분산이 모분산을 추정함에 있어 불편추정량이 되기 위해서이다. 이렇게 나눔으로써 표본분산의 기대치가 모분산과 같아진다.

파라미터(beta)와 모델에 대해 추정을 위해서 추가적인 가정이 또 필요하다. 이는 수식 3.1.에 대해서, **X의 기대값에 gaussian 분포를 따르는  error, Gaussian random variable; epsilon(![formular](https://render.githubusercontent.com/render/math?math=\epsilon\sim{N(0,\sigma^2)}))을 더하는 것**이다.

<div align="center">
<img src="imgs/eq_3_9.jpg" />
</div>

3.9를 바탕으로 beta hat을 아래와 같은 정규분포로 정의할 수 있다.

<div align="center">
<img src="imgs/eq_3_10.jpg" />
</div>

<div align="center">
<img src="imgs/eq_3_11.jpg" />
</div>

3.10은 mean vector와 분산-공분산행렬으로 표현할 수 있는 다항 정규 분포이며, yi와 yi-hat의 오차합인 (N-p-1)sigma-square는 sigma-square X N-p-1의 카이제곱 분포를 따르는 값이 된다.

이러한 분포적인 특징을 가지고 파라미터 beta-j를 위한 가설 및 신뢰구간을 검증하는데 사용한다.

====================================

### 카이제곱분포 - 한 집단의 분산

<a href="https://math100.tistory.com/44">https://math100.tistory.com/44</a>

분선의 특징을 확률분포로 만든 분포. 제곱된 값을 다루기 때문에 ![formular](https://render.githubusercontent.com/render/math?math=\chi^2)(chi;카이)로 나타낸다.

<div align="center">
<img src="imgs/chi-square_1.jpg" />
</div>

카이제곱분포는 집단의 분산을 추정하고 검정할 때 많이 사용되며, 제곱된 값을 사용하기 떄문에 양의 값만 존재한다. 일반적인 정규분포와 비교하면 아래와 같다.

<div align="center">
<img src="imgs/chi-square_2.jpg" />
</div>

카이제곱분포의 모양에서는 0과 가까운 경우 멀리 있는 것 보다 분포가 많은 것을 알 수 있다. 왜냐하면 데이터의 분산은 극단적인 경우보다 어느 정도인 경우가 많기 때문이다. 

그러나, 0과 가까운 경우에도 분포는 적다. 왜냐하면 데이터에는 어느 정도의 차이는 발생하기 떄문이다.

그리고 카이제곱분포는 표본의 수가 많을수록 넓직한 정규분포 형태가 된다.

<div align="center">
<img src="imgs/chi-square_3.jpg" />
</div>

카이제곱분포는 직접 확률을 구할 때 사용하는 분포가 아니라, 신뢰구간과 가설검정 및 그 밖의 여러 분석에서 사용되는 분포이다.

====================================

특정 계수 **bj=0이라는 가설을 검증**하기 위해, z값은 아래와 같이 계산된다.

<div align="center">
<img src="imgs/eq_3_12.jpg" />
</div>

vj는 p X p 정방행렬의 j번째 element이며, bj=0이라는 귀무가설 아래에 zj는 N-p-1 자유도를 가지는 t분포를 가지며, zj값이 큰 값을 띄면 귀무가설을 기각한다. 

만약 위의 식에서 sigma-hat이 sigma로 대체되면(sigma값을 알고있는경우?), zj는 표준정규분포를 따른다. 

Figure 3.3.의 정규분포와 t분포의 차이는 샘플의 수가 커지면 무시됨으로 정규분포를 대부분 사용한다.

그리고 종종 계수들을 모은 집합간의 유의성을 검증하기 위해서 F 분포(두 집단의 분산)를 사용한다.

i.e. k-level의 categorical value를 dummy encoding한 것과 하지 않은 두 그룹을 비교하는 경우.

<div align="center">
<img src="imgs/eq_3_13.jpg" />
</div>

0는 encoding 이전의 값들을, 1은 encoding 이후의 값들을 나타낸다.

F 분포는 bigger model(prefix;1)에서 추가된 파라미터 별로 잔차합의 변화를 측정한다. 그리고 F분포는 분산 sigma-sqaure에 의해 정규화 된다. 

F를 활용한 검증에서는 gaussian 가정 아래에 smaller model(prefix;0)가 맞다는 귀무가설을 세우고 F 분포는 ![formular](https://render.githubusercontent.com/render/math?math=F_{p_1-p_0,N-p_1-1})와 같은 자유도를 가지는 분포를 기진다. N의 크기가 크게되면 ![formular](https://render.githubusercontent.com/render/math?math=F_{p_1-p_0,N-p_1-1})의 사분위수들은 ![formular](https://render.githubusercontent.com/render/math?math=\chi^2_{p_1,p_0}/(p_1-p_0))로 나타낼 수 있다.

유사하게 bj를 1-2a의 신뢰수준을 가지는 신뢰구간을 아래와 같이 도출 할 수 있다.

<div align="center">
<img src="imgs/eq_3_14.jpg" />
<img src="imgs/eq_3_14_1.jpg" />
</div>

양극으로 95% 및 90%의 신뢰수준을 나타내는 신뢰구간을 표현하기 위한 z값은 위와 같다. 샘플의 크기 N이 극한으로 점차 증가한다면, 1-2a 신뢰수준으로 적절히 신뢰구간을 올바르게 추정할 수 있다.

위와 같은 방법으로 linear model에 있는 모든 beta들에 대해 신뢰구간을 아래와 같이 표현할 수 있게 된다.

<div align="center">
<img src="imgs/eq_3_15.jpg" />
</div>


### 3.2.2. The Gauss-Markov Theorem

Gauss-Markov 정리는 bias가 없는 상태에서 least square를 통한 추정이 가장 작은 선형추정의 MSE error를 가짐을 증명하는 정리이다.

그리고, dataset에 따라 unbiased한 경우도 있는데, 이를 적용하기 위해 biased하도록 제약을 굳이 가할 필요가 없다는 것도 보인다. 이런 biased estimate들에 대해서는 'ridge regression'에서 자세히 다루게 된다.

우리가 추정해야하는 Theta는 ![formular](https://render.githubusercontent.com/render/math?math=\Theta=a^T{\beta})이며, Theta-hat은 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_17.jpg" />
</div>

X가 고정되어있다고 생각하면, a^T는 constant variable이기 때문에 y에따라 반응하는 linear function으로 생각할 수 있다. 

<div align="center">
<img src="imgs/proof_3_1.jpg" />
</div>

X를 고정했을 때, Theta는 unbiased estimate 상태이며, 이는 Gauss-Markov Theorem에 따라 X를 고정했을 때의 Theta에 대한 분산을 3.19.와 같이 비교할 수 있다.

이는 a^t-beta-hat보다 나은 불편추정량은 없다는 것을 일컫는다.

====================================

#### About Gauss-Markov Theorem

<a href="https://blog.naver.com/yunjh7024/220880125898">https://blog.naver.com/yunjh7024/220880125898</a>

선형관계를 평가하기 위한 다양한 척도들이 존재(least square, likelihood function ...)하지만, Least Square 방식에서 가장 좋은 선형관계를 보여주기 위한 정리

단순선형 회귀분석을 사용하려면 오차변수인 '잔차'가 갖추어야할 네가지 조건이 있다.

네가지 조건을 만족해야 최소자승선(회귀선)은 예측치로 사용할 수 있다는 결론을 내릴 수 있으며, 이를 만족했을때 BLUE(Best Linear Unbiased Estimator)라고 부른다.

1. 오차변수의 기대값은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=E[\epsilon]=0)
2. 오차변수와 독립변수의 공분산은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=Cov[X,\epsilon]=0)
3. 오차변수의 분산은 일정한 상수이다. ![formular](https://render.githubusercontent.com/render/math?math=Var[\epsilon]=E[\epsilon^2]-E[\epsilon]^2=E[\epsilon^2]=\sigma^2)
4. 오차변수들 사이의 공분산은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=Cov[e_i,e_j]=0,foralli\neq{j})

====================================

최적의 회귀선 Theta를 추정하기 위한 추정량을 Theta-tilde로 하여 추정량에 대한 ESE를 아래와 같이 도출할 수 있다.

<div align="center">
<img src="imgs/eq_3_20.jpg" />
<img src="imgs/proof_3_2.jpg" />
</div>

그런데 우리는 X를 고정한 상태이며, estimator Theta-tilde는 불편추정량이기 때문에 E(Theta-tilde)=Theta임으로 우측의 bias-square는 0가 된다.

====================================

#### Estimation, Bias, Mean Squared Error

<a href="http://www.statslab.cam.ac.uk/Dept/People/djsteaching/S1B-15-02-estimation-bias-4.pdf">http://www.statslab.cam.ac.uk/Dept/People/djsteaching/S1B-15-02-estimation-bias-4.pdf</a>

====================================

그런데, biased estimator 또한 존재할 수 있다. 이때는 수식 3.20.에 따라 variance의 감소를 위해 약간의 bias를 희생해야 할것이다. 

least sqaure의 계수를 줄이거나 또는 0으로 설정하는 방법들은 biased estimate를 만드는 방법이고,  이는 subset selection 과 ridge regression을 통해서 살펴본다. 그리고 실용적으로 모델을 사용하기 위해서 일부로 biased한 형태로 왜곡하기도 한다.

마지막으로 MSE는 아래의 수식과 같이 나타나는데 이를 위한 증명과정은 아래와 같다.

<div align="center">
<img src="imgs/eq_3_22.jpg" />
<img src="imgs/proof_2_4.jpg" />
</div>

### 3.2.3. Multiple Regression from Simple Univariate Regression

linear regression을 수행하는데 있어서 p의 값을 1로 하는 경우(x의 개수가 1개)를 Simple Univariate Regression이라고 한다. 이제는 p를 1개 이상의 값을 가지도록 확장하려고 한다.

우선 만약 우리가 univariate model을 아래와 같이 가진다고 하면 추정량 beta-hat과 각 target value의 잔차를 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_23.jpg" />
<img src="imgs/eq_3_24.jpg" />
<img src="imgs/eq_3_25.jpg" />
<img src="imgs/eq_3_26.jpg" />
</div>

<x,y>는 x와 y벡터 간의 내적을 의미하며, 이를 활용하여 3.24.를 3.26.의 식으로 바꿔서 표현할 수 있다.

다음으로 p의 개수를 늘이는데, 각각의 p컬럼들이 '직교'한다고 생각하자(<xj,xk>=0, all j!=k; 벡터가 직교하는 경우에는 내적이 0이다). 추정해야 하는 beta-hat-j의 수가 많아지더라도 이는 ![formular](https://render.githubusercontent.com/render/math?math=\hat{\beta}_j=<x_j,y>/<x_j,x_j>) 단일 p값에 대한 추정치와 동일해진다. 다시 말해 input vector들이 직교한다면 개별 feature의 파라미터 추정에 상호간의 간섭이 없다는 것을 의미한다.

이런 직교성은 실제 데이터셋에 절대 일어날수 없다. 그래서 이들을 몇개의 가정을 더하여 직교성을 띄도록 만들어줄 것이다.

이제 또 더 나아가 우리의 linear model에는 절편이 존재하고 input을 1개(x0)만 있다고 가정하자. 그렇다면 x1의 추정량 beta-hat-1은 아래와 같다.

<div align="center">
<img src="imgs/eq_3_27.jpg" />
</div>

(![formular](https://render.githubusercontent.com/render/math?math=\bar{x}=\sum_i{x_i}/N) and ![formular](https://render.githubusercontent.com/render/math?math=1=x_0))

3.27은 3.26의 간단한 회귀식에서 2가지 항목을 적용하여 도출되었다.

1. 잔차 z=x-x-bar-1을 생성하기 위해 x0에 x를 절편이 없는 단순회귀식으로 투영
2. beta-hat-1를 계산하기 위해 잔차 z에 y를 절편이 없는 단순회귀식으로 투영

'regress b on a'는 a에 b를 절편이 없는 단순회귀식으로 투영한 상태를 말하며, 계수 ![formular](https://render.githubusercontent.com/render/math?math=\hat{\gamma}=<a,b>/<a,a>)와 잔차 vector ![formular](https://render.githubusercontent.com/render/math?math=b-\hat{\gamma}a)를 도출할 수 있다. 그리고 이를 a의 측면에서 '직교화'되었다고 표현한다.

<div align="center">
<img src="imgs/fig_3_4.jpg" />
</div>
