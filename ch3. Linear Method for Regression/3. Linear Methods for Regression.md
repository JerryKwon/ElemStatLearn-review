# 3. Linear Method for Regression

## caution

except for 3.7. ~ the end

## 3.1. Introduction

Linear Method는 과거부터 현재까지 널리 쓰이는 모델이다. 그리고, input이 output에 어떤 영향을 미치는지 설명하는 것이 쉬우며, training case가 적거나 signal-to-noise 비율이 낮거나 sparse data인 경우에는 non-linear 모델보다 성능이 좋다.
마지막으로, input 값들의 단위를 변환시키는데 사용되며 이를 'basis-function method'라고 부른다.

## 3.2. Linear Regression Models and Least Squares

기본적인 단순선형회귀에서 다항선형회귀로 그리고 output의 개수를 늘린 다항선형회귀로 어떻게 확장할 수 있는 지 탐구하는 절.

<div ailgn="center">
<img src="imgs/eq_3_1.jpg" />
</div>

선형모델은 p개의 feature(![formular](https://render.githubusercontent.com/render/math?math=X^T=(X_1,X_2,...,X_p)))를 가지는 위의 수식으로 표현할 수 있다.

Xj로 위치할 수 있는 Data source

* quantitative inputs
* transformations of quantitiative inputs, such as log, squared-root or square
* basis expansions, such as ![formular](https://render.githubusercontent.com/render/math?math=X_2=X_1^2,X_3=X_1^3), leading to a polynomial representation
* numeric or "dummy" coding of the levels of quantitative inputs
* interactions between variables

Trainin data는 (x1,y1)의 쌍으로 ... (Xn,yn)과 같이 N개의 datapoint로 구성되어 있으며, 하나의 datapoint i에 대해서는 (xi1,xi2,...,xip)와 같이 p개의 column 값으로 구성된다. 그리고 estimation을 통해 추측해야 하는 beta의 값 또한 beta = (b0,b1,...bp)의 와 같이 구성된다.

beta를 estimation 하기 위해 가장 유명한 접근 방법은 RSS 방식이며, 이는 아래와 같다.

<div ailgn="center">
<img src="imgs/eq_3_2.jpg" />
</div>

training observation (xi,yi)는 모집단으로부터 random(임의로)하게 추출되는 것이 바람직한 방법이다. **그러나 xi가 임의로 추출되지 않더라도 yi가 주어진 xi에 대해 conditionally independent하다면 여전히 RSS 방법은 유효하다.**

====================================

### conditional independent

<a href="http://norman3.github.io/prml/docs/chapter08/2.html">http://norman3.github.io/prml/docs/chapter08/2.html</a>

세 개의 변수 a, b, c가 있다.
* 이 때 b와 c가 주어졌을 때 a에 대한 조건부확률을 계산해보자.
* a가 b에 독립적이라면 다음과 같은 식이 성립한다.
    
    <img src="https://latex.codecogs.com/svg.latex?p(a|b,c)=p(a|c) \qquad{(8.20)}" title="p(a|b,c)=p(a|c) \qquad{(8.20)}" />

    위의 경우를 c가 주어졌을 때 a는 b에 대해 조건부 독립이라고 표현한다.

* 이런 상황에서 c가 주어진 상태의 a와 b의 결합 분포는 다음과 같다.

    <img src="https://latex.codecogs.com/svg.latex?p(a,b|c) = p(a|b,c)p(b|c)=p(a|c)p(b|c) \qquad{(8.21)}" title="p(a,b|c) = p(a|b,c)p(b|c)=p(a|c)p(b|c) \qquad{(8.21)}" />

    이를 조건부 독립이라고 하고, 다음과 같이 표기한다.

    <img src="https://latex.codecogs.com/svg.latex?a\perp\!\!\!\!\!\perp b\;|\;c \qquad{(8.22)}" title="a\perp\!\!\!\!\!\perp b\;|\;c \qquad{(8.22)}" />

확률 변수의 조건부 독립여부는 중요한 속성이다.
* 사용되는 모델을 단순화시킬 수 있고,
* 학습에 필요한 연상량을 낮출 수 있다.

====================================

eq 3.2.는 linear model 3.1.에서 아무런 가정이 없는 수식이지만 Least square 방식으로 어느 정도는 fitting 가능하다.

X를 N X (p+1) matrix로, y를 N vector로 구성된 훈련셋의 output으로 따져볼 때, 어떻게 3.2. 식을 최소화할 수 있을까?

<div align="center">
<img src="imgs/eq_3_3.jpg" />
<img src="imgs/eq_3_4.jpg" />
</div>

3.3. 식을 beta에 대한 2차원식으로 나타내면, RSS를 최소로하는 beta를 계산하기 위해 beta에 대한 1차 및 2차 편미분을 식으로 나타내면 3.4.와 같다.

**X를 full column rank로 가정**하면 ![formular](https://render.githubusercontent.com/render/math?math=X^TX)는 양의 값을 가지고 1차 미분에 대해 0으로 하는 식은 아래와 같다. 

<div align="center">
<img src="imgs/eq_3_5.jpg" />
<img src="imgs/eq_3_6.jpg" />
</div>

그리고 이를 unique하게 하는 beta의 값은 위와 같다.

<div align="center">
<img src="imgs/eq_3_7.jpg" />
</div>

input vector x0에 대한 예측값은 ![formular](https://render.githubusercontent.com/render/math?math=\hat{f}(x_0)=(1:x_0)^T\hat{\beta})로 나타낼 수 있으며 training input으로 feat된 값은 위의 수식과 같다.

![formular](https://render.githubusercontent.com/render/math?math=H=X(T^TX)^{-1}X^Ty)는 y에 hat을 붙이기 위한 Matrix로 "hat" matrix라고도 불린다.

<div align="center">
<img src="imgs/fig_3_2.jpg" />
</div>

위의 그림은 least square 추정을 기하학적으로 나타내었을 때의 그림이며, x0는 1로 가정한 상태에서 x1과 x2 그리고 y에 대한 3차원 공간으로 투영했다. x1과 x2의 부분공간에 y의 직선이 추정치 y hat과 직교하도록 RSS를 최소화하는 beta를 찾아야 한다.

====================================

### rank and dimension in Linear algebra

<a href="https://losskatsu.github.io/linear-algebra/rank-dim/#">https://losskatsu.github.io/linear-algebra/rank-dim/#</a>

X의 컬럼들이 선형 독립이 아닌 경우(bot of full rank)가 발생할 수 있다(i.e. x2 = 3x1). 이 경우에는 ![formular](https://render.githubusercontent.com/render/math?math=X^TX)는 singular(역행렬을 가지지 않음)이며, least square의 beta hat은 unique한 값을 가질 수 없게 된다. 그러나 y hat은 여전히 X column들의 부분공간에 투영된다; X의 컬럼 벡터들의 관점에서 투영되는 것을 다른 방법으로 설명할 수 있다.

====================================

이러한 non-full rank case(컬럼간에 독립이 아닌 경우)는 qualitative input이 중복적으로 반영될 때 나타나며, 이를 위한 일반적인 방법은 반영을 새로하거나 X의 중복 컬럼을 제거하는 것이다. 

그리고 Rank Deficiency(training sample의 수보다 컬럼의 수가 더 많은 경우)의 경우에는 feature를 filtering이나 정규화를 반영한 fitting을 통해 해결한다.

**벡터 X의 컬럼이 상호 독립적이라는 가정을 가지고, beta hat을 추정하는데 yi간에 상관관계가 없고 ![formular](https://render.githubusercontent.com/render/math?math=\sigma^2)의 분산을 가지고 xi가 고정(non random)되어 있다고 가정하자.** Least sqaure 파라미터 추정을 위한 분산-공분산 matrix는 3.6과 아래의 3.8 수식을 통해 도출된다.

<div align="center">
<img src="imgs/eq_3_8.jpg" />
</div>

====================================

### 분산-공분산행렬 [variance-covariance matrix]

<a href="https://adioshun.gitbooks.io/statics-with-r/content/variance-covariance-matrix.html">https://adioshun.gitbooks.io/statics-with-r/content/variance-covariance-matrix.html</a>

k feature에 대한 정방행렬(k X k 행렬)을 각 변수의 분산과 공분산으로 채운 것.
* 대각행렬: 각 변수의 분산
* 이외의 값: 변수 사이의 공분산

====================================

3.6.과 3.8.을 가지고 yi에 대한 분산은 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_8_1.jpg" />
</div>

분모로 N 대신에 N-p-1를 하는 이유는 yi의 분산 추정량 sigma-hat-sqaure를 sigma-square의 불편추정량으로 만들기 위함이다(![formular](https://render.githubusercontent.com/render/math?math=\sigma^2:E(\hat{\sigma}^2)=\sigma^2)). 

====================================

### 불편추정량

<a href="http://www.aistudy.co.kr/math/estimate_lee.htm">http://www.aistudy.co.kr/math/estimate_lee.htm</a>

불편추정량은 편의가 없는 추정량이며, 이는 추정량이 모수에 대하여 큰 차이를 보이지 않고 어느 한쪽으로 치우침이 없다는 의미. 그러므로 불편추정량은 모수에 대하여 올바른 추정량이라는 의미이다.

i.e. 표본평균 및 표본비율에 대해서는 표본에 대한 기대값이 모수의 추정량과 같으므로 불편추정량이다.

====================================

표본분산은 편차의 제곱합을 N-1로 나누는데, 이는 표본분산이 모분산을 추정함에 있어 불편추정량이 되기 위해서이다. 이렇게 나눔으로써 표본분산의 기대치가 모분산과 같아진다.

파라미터(beta)와 모델에 대해 추정을 위해서 추가적인 가정이 또 필요하다. 이는 수식 3.1.에 대해서, **X의 기대값에 gaussian 분포를 따르는  error, Gaussian random variable; epsilon(![formular](https://render.githubusercontent.com/render/math?math=\epsilon\sim{N(0,\sigma^2)}))을 더하는 것**이다.

<div align="center">
<img src="imgs/eq_3_9.jpg" />
</div>

3.9를 바탕으로 beta hat을 아래와 같은 정규분포로 정의할 수 있다.

<div align="center">
<img src="imgs/eq_3_10.jpg" />
</div>

<div align="center">
<img src="imgs/eq_3_11.jpg" />
</div>

3.10은 mean vector와 분산-공분산행렬으로 표현할 수 있는 다항 정규 분포이며, yi와 yi-hat의 오차합인 (N-p-1)sigma-square는 sigma-square X N-p-1의 카이제곱 분포를 따르는 값이 된다.

이러한 분포적인 특징을 가지고 파라미터 beta-j를 위한 가설 및 신뢰구간을 검증하는데 사용한다.

====================================

### 카이제곱분포 - 한 집단의 분산

<a href="https://math100.tistory.com/44">https://math100.tistory.com/44</a>

분선의 특징을 확률분포로 만든 분포. 제곱된 값을 다루기 때문에 ![formular](https://render.githubusercontent.com/render/math?math=\chi^2)(chi;카이)로 나타낸다.

<div align="center">
<img src="imgs/chi-square_1.jpg" />
</div>

카이제곱분포는 집단의 분산을 추정하고 검정할 때 많이 사용되며, 제곱된 값을 사용하기 떄문에 양의 값만 존재한다. 일반적인 정규분포와 비교하면 아래와 같다.

<div align="center">
<img src="imgs/chi-square_2.jpg" />
</div>

카이제곱분포의 모양에서는 0과 가까운 경우 멀리 있는 것 보다 분포가 많은 것을 알 수 있다. 왜냐하면 데이터의 분산은 극단적인 경우보다 어느 정도인 경우가 많기 때문이다. 

그러나, 0과 가까운 경우에도 분포는 적다. 왜냐하면 데이터에는 어느 정도의 차이는 발생하기 떄문이다.

그리고 카이제곱분포는 표본의 수가 많을수록 넓직한 정규분포 형태가 된다.

<div align="center">
<img src="imgs/chi-square_3.jpg" />
</div>

카이제곱분포는 직접 확률을 구할 때 사용하는 분포가 아니라, 신뢰구간과 가설검정 및 그 밖의 여러 분석에서 사용되는 분포이다.

====================================

특정 계수 **bj=0이라는 가설을 검증**하기 위해, z값은 아래와 같이 계산된다.

<div align="center">
<img src="imgs/eq_3_12.jpg" />
</div>

vj는 p X p 정방행렬의 j번째 element이며, bj=0이라는 귀무가설 아래에 zj는 N-p-1 자유도를 가지는 t분포를 가지며, zj값이 큰 값을 띄면 귀무가설을 기각한다. 

만약 위의 식에서 sigma-hat이 sigma로 대체되면(sigma값을 알고있는경우?), zj는 표준정규분포를 따른다. 

Figure 3.3.의 정규분포와 t분포의 차이는 샘플의 수가 커지면 무시됨으로 정규분포를 대부분 사용한다.

그리고 종종 계수들을 모은 집합간의 유의성을 검증하기 위해서 F 분포(두 집단의 분산)를 사용한다.

i.e. k-level의 categorical value를 dummy encoding한 것과 하지 않은 두 그룹을 비교하는 경우.

<div align="center">
<img src="imgs/eq_3_13.jpg" />
</div>

0는 encoding 이전의 값들을, 1은 encoding 이후의 값들을 나타낸다.

F 분포는 bigger model(prefix;1)에서 추가된 파라미터 별로 잔차합의 변화를 측정한다. 그리고 F분포는 분산 sigma-sqaure에 의해 정규화 된다. 

F를 활용한 검증에서는 gaussian 가정 아래에 smaller model(prefix;0)가 맞다는 귀무가설을 세우고 F 분포는 ![formular](https://render.githubusercontent.com/render/math?math=F_{p_1-p_0,N-p_1-1})와 같은 자유도를 가지는 분포를 기진다. N의 크기가 크게되면 ![formular](https://render.githubusercontent.com/render/math?math=F_{p_1-p_0,N-p_1-1})의 사분위수들은 ![formular](https://render.githubusercontent.com/render/math?math=\chi^2_{p_1,p_0}/(p_1-p_0))로 나타낼 수 있다.

유사하게 bj를 1-2a의 신뢰수준을 가지는 신뢰구간을 아래와 같이 도출 할 수 있다.

<div align="center">
<img src="imgs/eq_3_14.jpg" />
<img src="imgs/eq_3_14_1.jpg" />
</div>

양극으로 95% 및 90%의 신뢰수준을 나타내는 신뢰구간을 표현하기 위한 z값은 위와 같다. 샘플의 크기 N이 극한으로 점차 증가한다면, 1-2a 신뢰수준으로 적절히 신뢰구간을 올바르게 추정할 수 있다.

위와 같은 방법으로 linear model에 있는 모든 beta들에 대해 신뢰구간을 아래와 같이 표현할 수 있게 된다.

<div align="center">
<img src="imgs/eq_3_15.jpg" />
</div>


### 3.2.2. The Gauss-Markov Theorem

Gauss-Markov 정리는 bias가 없는 상태에서 least square를 통한 추정이 가장 작은 선형추정의 MSE error를 가짐을 증명하는 정리이다.

그리고, dataset에 따라 unbiased한 경우도 있는데, 이를 적용하기 위해 biased하도록 제약을 굳이 가할 필요가 없다는 것도 보인다. 이런 biased estimate들에 대해서는 'ridge regression'에서 자세히 다루게 된다.

우리가 추정해야하는 Theta는 ![formular](https://render.githubusercontent.com/render/math?math=\Theta=a^T{\beta})이며, Theta-hat은 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_17.jpg" />
</div>

X가 고정되어있다고 생각하면, a^T는 constant variable이기 때문에 y에따라 반응하는 linear function으로 생각할 수 있다. 

<div align="center">
<img src="imgs/proof_3_1.jpg" />
</div>

X를 고정했을 때, Theta는 unbiased estimate 상태이며, 이는 Gauss-Markov Theorem에 따라 X를 고정했을 때의 Theta에 대한 분산을 3.19.와 같이 비교할 수 있다.

이는 a^t-beta-hat보다 나은 불편추정량은 없다는 것을 일컫는다.

====================================

#### About Gauss-Markov Theorem

<a href="https://blog.naver.com/yunjh7024/220880125898">https://blog.naver.com/yunjh7024/220880125898</a>

선형관계를 평가하기 위한 다양한 척도들이 존재(least square, likelihood function ...)하지만, Least Square 방식에서 가장 좋은 선형관계를 보여주기 위한 정리

단순선형 회귀분석을 사용하려면 오차변수인 '잔차'가 갖추어야할 네가지 조건이 있다.

네가지 조건을 만족해야 최소자승선(회귀선)은 예측치로 사용할 수 있다는 결론을 내릴 수 있으며, 이를 만족했을때 BLUE(Best Linear Unbiased Estimator)라고 부른다.

1. 오차변수의 기대값은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=E[\epsilon]=0)
2. 오차변수와 독립변수의 공분산은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=Cov[X,\epsilon]=0)
3. 오차변수의 분산은 일정한 상수이다. ![formular](https://render.githubusercontent.com/render/math?math=Var[\epsilon]=E[\epsilon^2]-E[\epsilon]^2=E[\epsilon^2]=\sigma^2)
4. 오차변수들 사이의 공분산은 0이다. ![formular](https://render.githubusercontent.com/render/math?math=Cov[e_i,e_j]=0,foralli\neq{j})

====================================

최적의 회귀선 Theta를 추정하기 위한 추정량을 Theta-tilde로 하여 추정량에 대한 ESE를 아래와 같이 도출할 수 있다.

<div align="center">
<img src="imgs/eq_3_20.jpg" />
<img src="imgs/proof_3_2.jpg" />
</div>

그런데 우리는 X를 고정한 상태이며, estimator Theta-tilde는 불편추정량이기 때문에 E(Theta-tilde)=Theta임으로 우측의 bias-square는 0가 된다.

====================================

#### Estimation, Bias, Mean Squared Error

<a href="http://www.statslab.cam.ac.uk/Dept/People/djsteaching/S1B-15-02-estimation-bias-4.pdf">http://www.statslab.cam.ac.uk/Dept/People/djsteaching/S1B-15-02-estimation-bias-4.pdf</a>

====================================

그런데, biased estimator 또한 존재할 수 있다. 이때는 수식 3.20.에 따라 variance의 감소를 위해 약간의 bias를 희생해야 할것이다. 

least sqaure의 계수를 줄이거나 또는 0으로 설정하는 방법들은 biased estimate를 만드는 방법이고,  이는 subset selection 과 ridge regression을 통해서 살펴본다. 그리고 실용적으로 모델을 사용하기 위해서 일부로 biased한 형태로 왜곡하기도 한다.

마지막으로 MSE는 아래의 수식과 같이 나타나는데 이를 위한 증명과정은 아래와 같다.

<div align="center">
<img src="imgs/eq_3_22.jpg" />
<img src="imgs/proof_2_4.jpg" />
</div>

### 3.2.3. Multiple Regression from Simple Univariate Regression

linear regression을 수행하는데 있어서 p의 값을 1로 하는 경우(x의 개수가 1개)를 Simple Univariate Regression이라고 한다. 이제는 p를 1개 이상의 값을 가지도록 확장하려고 한다.

우선 만약 우리가 univariate model을 아래와 같이 가진다고 하면 추정량 beta-hat과 각 target value의 잔차를 아래와 같이 나타낼 수 있다.

<div align="center">
<img src="imgs/eq_3_23.jpg" />
<img src="imgs/eq_3_24.jpg" />
<img src="imgs/eq_3_25.jpg" />
<img src="imgs/eq_3_26.jpg" />
</div>

<x,y>는 x와 y벡터 간의 내적을 의미하며, 이를 활용하여 3.24.를 3.26.의 식으로 바꿔서 표현할 수 있다.

다음으로 p의 개수를 늘이는데, 각각의 p컬럼들이 '직교'한다고 생각하자(<xj,xk>=0, all j!=k; 벡터가 직교하는 경우에는 내적이 0이다). 추정해야 하는 beta-hat-j의 수가 많아지더라도 이는 ![formular](https://render.githubusercontent.com/render/math?math=\hat{\beta}_j=<x_j,y>/<x_j,x_j>) 단일 p값에 대한 추정치와 동일해진다. 다시 말해 input vector들이 직교한다면 개별 feature의 파라미터 추정에 상호간의 간섭이 없다는 것을 의미한다.

이런 직교성은 실제 데이터셋에 절대 일어날수 없다. 그래서 이들을 몇개의 가정을 더하여 직교성을 띄도록 만들어줄 것이다.

이제 또 더 나아가 우리의 linear model에는 절편이 존재하고 input을 1개(x0)만 있다고 가정하자. 그렇다면 x1의 추정량 beta-hat-1은 아래와 같다.

<div align="center">
<img src="imgs/eq_3_27.jpg" />
</div>

(![formular](https://render.githubusercontent.com/render/math?math=\bar{x}=\sum_i{x_i}/N) and ![formular](https://render.githubusercontent.com/render/math?math=1=x_0))

3.27은 3.26의 간단한 회귀식에서 2가지 항목을 적용하여 도출되었다.

1. 잔차 z=x-(x-bar-1)을 생성하기 위해 x0에 x를 절편이 없는 단순회귀식으로 투영
2. beta-hat-1를 계산하기 위해 잔차 z에 y를 절편이 없는 단순회귀식으로 투영

'regress b on a'는 a에 b를 절편이 없는 단순회귀식으로 투영한 상태를 말하며, 계수 ![formular](https://render.githubusercontent.com/render/math?math=\hat{\gamma}=<a,b>/<a,a>)와 잔차 vector ![formular](https://render.githubusercontent.com/render/math?math=b-\hat{\gamma}a)를 도출할 수 있다. 그리고 이를 a의 측면에서 '직교화'되었다고 표현한다.

<div align="center">
<img src="imgs/fig_3_4.jpg" />
</div>

위의 figure는 x0=1이라는 특정 point에 고정시켜둔 채로, x1과 x2벡터에 대해 직교화하는 절차를 묘사한다. 직교화는 x1과 x2로 인해 투영되는 부분공간을 변화시키지 않으며, 단지 z와 같은 직교하는 기저벡터를 생성한다. 

이러한 일련의 절차를 일반화하여 각 column을 p로 z 벡터를 직교하는 벡터로 하면 Algorithm 3.1.(Gram-Schmidt procedure)과 같이 나타낼 수 있다. 

<div align="center">
<img src="imgs/algo_3_1.jpg" />
<img src="imgs/eq_3_28.jpg" />
</div>

해당 알고리즘의 결과로 얻을 수 있는 p별 추정치 beta는 위와 같다. step2의 zj를 구하는 식을 재정의하면, xj는 zj와 zk가 선형적으로 결합한 식임을 알 수 있다. 그리고, 모든 zj들이 직교하기 때문에 이들을 활용하여 X의 컬럼에 대한  기저를 생성할 수 있게 된다. 이렇게 생성된 기저 공간에 least square y-hat을 투영한다.

만약, 특정 컬럼 p가 다른 컬럼과 놓은 상관관계를 띈다면 zp는 0에 가까운 값으로 나타나고, beta-hat-p는 불안정해진다. 실제 데이터에서는 컬럼간의 상관관계가 다분히 발생하며, 이때는 Table 3.2.와 같이 z-score를 계산할 때 값이 작아질 것이고, 작은 z-score를 가지는 값들 중에 하나를 삭제해야 할 것이다.

이런 상관관계를 띌 때, 대안으로 사용되는 추정량을 구하는 식은 아래와 같다. 

<div align="center">
<img src="imgs/eq_3_29.jpg" />
</div>

다시말하면, 추정량은 잔차 벡터 zp의 길이에 관련이 있고, 이는 특정 컬럼에 대한 xp가 다른 컬럼들 xk들에 의해 얼마나 설명되어지지 않는지를 나타낸다. 

### 3.2.4. Multiple Outputs

이제 한 datapoint의 output을 한 개가 아닌 '여러 개'로 확장하여 생각하자(Y1, Y2,..., Yk). 이때의 linear model에 대한 방정식은 아래와 같다. 

<div align="center">
<img src="imgs/eq_3_34_35.jpg" />
<img src="imgs/eq_3_36.jpg" />
</div>

**Y is N X K; X is N X (p+1); B is (p+1) X K; E is N X K와 같은 형태로 행렬이 구성**되어 있다. k번째 결과를 위한 계수(beta-hat-k)는 input(x0,x1,...,xp)들에 대해 yk와만 관련하여 계산되는 추정치이다.

<div align="center">
<img src="imgs/eq_3_37_38.jpg" />
</div>

'multiple output'이더라도, 다른 output들 간에는 영향을 미치지 않는다. 

그리고 만약 error(![formular](https://render.githubusercontent.com/render/math?math=\epsilon=(\epsilon_1,...,\epsilon_k)))가 상관관계가 있다면 3.37의 식을 3.40와 같이 수정해야 한다.  

<div align="center">
<img src="imgs/eq_3_40.jpg" />
</div>

## 3.3. Subset Selection

<a href="http://www.math.udel.edu/~dguillot/teaching/MATH829/lectures-handout/5-MATH829-Subset-selection-handout.pdf">http://www.math.udel.edu/~dguillot/teaching/MATH829/lectures-handout/5-MATH829-Subset-selection-handout.pdf</a>

전통적인 선형회귀의 성능을 향상시키는 방법 중 'interpretation'과 관련있는 'Subset Selection'방법을 설명하는 절.

전통적인 선형회귀는 bias가 낮고 variance가 높은 모델이다. 그런데, bias-variance trade-off에 의해 약간의 bias를 희생하면 variance를 낮출 수 있다. 이 방법에는 아래와 같이 두가지 방법이 있다.

1. prediction accuracy: 전통적인 least square 방식이 bias가 낮고 variance가 큰 형태이다. prediction accuracy는 컬럼을 줄이거나 특정 계수를 0으로 맞춤으로써 향상시킬수 있다. 이는 약간의 bias를 희생함으로써 variance를 낮출 수 있고 이 때문에 전반적인 예측 정확도가 높아지는 것이다.

2. interpretation: 예측시에 많은 수의 feature들을 가지고 있다면, target value에 강한 영향을 주는 feature들을 subset 형태로 추출하여 예측에 사용하는 경우가 있다. 이는 작게 영향을 미치는 일부 feature들을 배제하는 방법이다.

<div align="center">
<img src="imgs/fig_3_6.jpg" />
</div>

### 3.3.1. Best-Subset Selection.

k-size의 서브셋 구성을 RSS를 최소화하는 구성으로 하는 것이다. 이를 효율적으로 접근하는 방법(leaps and bounds procedure)에 따르면 적절한 p의 수는 30 or 40정도의 수를 가져야 한다.

그렇다면 p의 개수인 k를 어떻게 결정해야 할까?
많은 방법 중 EPE의 추정값을 최소로하는 모델을 고르는 방법이다. 

### 3.3.2. Forward-and Backward-Stepwise Selection

모든 가능한 subset들 중에서 찾는 방식 보다, 더 좋은 결과를 이끌어내는 subset들을 순차적으로 결정하는 것이다. 

Forward 방식은 column을 가지고 있지 않는 상태에서 'greedy algorithm'방식으로 컬럼을 찾는 것이다. 이 방식은 계산부하가 많은 것처럼 보이지만 앞서 언급한 QR분해를 활용하면 다음 컬럼 후보군을 빠르게 선정할 수 있다. 

이 방식이 Best-Subset 방식보다 주로 사용되는 이유는

* Computational: p가 큰 경우, Best-Subset 방식은 모든 p에 대한 subset을 만드는 것이 부하가 크다.

* Statistical: Best-Subset 방식은 특정 사이즈에 대해 최적읜 best subset을 선택함으로 variance가 높아지지만, forward stepwise 방식은 제한적인 방법으로 search를 진행함으로 variance를 낮추는 방식으로 search가 가능하다. 

Backward 방식은 모든 컬럼을 포함하는 상태에서 하나씩 덜어내는 것이다. 컬럼을 제외하는 기준은 가장 작은 Z-score를 배제하는 것이다. 그러나 이는 datapoint의 개수 N이 컬럼의 수 p보다 큰 경우에만 사용가능하다(forward 방식은 언제나 사용 가능).

#### hybrid stepwise-selection

이는 특정 linear model에 있어서 forward와 backward 방식을 동시에 사용하여 최적의 컬럼을 선정하는 방식이다. criterion은 AIC 값을 최저로하는 컬럼을 추가하고 더하는 방식으로 진행한다. 

====================================

#### **AIC score**

<a href="https://rpago.tistory.com/15">https://rpago.tistory.com/15</a>

회귀모델의 변수를 선정하는데 있어서 p값(변수의 개수)를 활용하는 것은 논란이 있다. 따라서, criterion 기준의 전수조사 방식을 추천하는데, 여기서 '기준'을 AIC, BIC, Cp를 말한다. 

* ![formular](https://render.githubusercontent.com/render/math?math=AIC=-2log(\mathcal{L})+2p)
* ![formular](https://render.githubusercontent.com/render/math?math=BIC=-2log(\mathcal{L})+plog(n))

p: 변수의 개수, n: 데이터의 개수

선형회귀에서는,

* ![formular](https://render.githubusercontent.com/render/math?math=AIC=nlog(RSS/n)+2p)
* ![formular](https://render.githubusercontent.com/render/math?math=BIC=nlong(RSS/n)+plog(n))

변수가 많은 모델의 경우 RSS는 작아진다(컬럼이 많아지면 오차에 대한 설명력이 더 높아지기 때문). 결국 AIC, BIC를 최소화 한다는 것은 **'우도(likelihood)를 가장 크게 하는 동시에, 변수 개수는 가장 적은 최적의 모델(parsimonious & explainable)'** 을 의미한다. 

Bias는 변수를 제거하면서 생기는 오류, Variance는 변수가 증가하면서 생기는 오류이다. 따라서 이를 AIC와 BIC에 대입하면 좌변을 bias, 우변을 variance로 부를 수 있다. 

<div align="center">
<img src="imgs/princlple of parsimony.png" />
</div>

====================================

### 3.3.3. Forward-Stagewise Regression

forward-stepwise 방식보다 더 제약이 가해진 방법. 
* 시작시, forward-stepwise 방식처럼 시작한다(intercept to y-bar;'y값의 평균', features from 0). 
* 각 단계별로 현재 잔차(y)에서 가장 관련된 변수를 찾아낸다
* 선택된 변수로 잔차의 선형 회귀 계수를 계산하고, current coefficient에 추가한다. 
* 이를 더이상 잔차(y)와 변수간에 상관관계가 등장하지 않을 때까지 지속한다. 

특징.

1. Unlike forward-stepwise regression, none of the other variables are adjusted when a term is added to the model.
2. The process can take more than p steps to reach the least squares fit.
3. Historically, forward stagewise regression has been dismissed as being ineffcient.
4. However, it can be quite competitive, especially in very high-dimensional problems.


